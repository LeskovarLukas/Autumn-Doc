\chapter{Simultaneous Localization and Mapping}
\label{chapter:slam}

\textbf{Author: Lukas Leskovar} 

%frame of reference klingt nicht so gut
The problem of localizing as well as navigating a system through a completely or partly unknown environment without any external coordinate system (i.e. GPS, Optical Beacon Tracking, etc.) has proven itself to be one of the most complex and yet fundamental topics in many scientific research fields with robotics being most prominent. The main approach to this problem is Simultaneous Localization and Mapping (SLAM) which dates back to the mid 1980s. Back then the first solutions based on Extended Kalman Filters or Rao-Blackwellised Filters were formulated. \footcite{durrantSlam2006}  \footcite{cadenaSlamFuture2016}

To date the topic of SLAM has matured, algorithms have gotten more reliable and robust and are utilized in many industries. Applications for SLAM range from Navigating a Mars Rover over autonomous cars or warehouse robots to simple household appliances like vacuum cleaners. 

As one major topic of this thesis is robot navigation in a GPS-denied area as well as mapping of such environment, different modern SLAM approaches and solutions utilized by the project team are discussed in this chapter.

\section{Localization}
%state estimation beschreiben dass pose sowie andere informationen wie geschw. usw. erfasst werden. 
Localization or state-Estimation aims to reconstruct the state of a system using interoceptive measurements (e.g. acceleration, velocity, etc.) as well as an exteroceptive model (e.g. position and orientation) of the system. \footcite{barfootStateEstimation2017}
Put into the context of mobile robotics this means that in order to perform comprehensive localization of a mobile robot a sensor fusion between on-board sensors and an external coordinate system or map ought to be performed as solely relying on incremental sensors for odometry would quickly result in large accumulated errors.
%Exemplary for such a robot would be a drone utilizing a Inertial Measurement Units (IMU) odometry as well as GPS positional data to estimate its current position.
Exemplary for such a system would be an industrial robot utilizing a construction plan to correct errors by wheel-encoded odometry as well as to navigate through a factory. 

\section{Mapping}
Contrary to localization or state-estimation, in mapping problems the current pose of the system is known while its environment remains uncharted.
Therefore stand-alone mapping aims to generate a model of its environment by evaluating sensor readings of environmental features as well as the systems pose to reconstruct aforementioned features in a global reference frame. 
Mapping applications often combine technologies typically found in other scientific fields such as photogrammetry or computer vision.
Another example would be a drone computing camera images and GPS positional data with a photogrammetric algorithm to reconstruct a 3D-Model of a building. 


\section{Localization and Mapping}
The aforementioned technologies on their own are considered rather simplistic problems as either the environments map is given or reliable pose-estimation can be provided. While most applications meet either of these criteria with pre-built maps or GPS being available, in some environments such as indoors, mineshafts or outer space both the systems pose and environment remain uncertain and need to be determined simultaneously hence the name Simultaneous Localization and Mapping. 
To summarize, the crux of the SLAM problem, as described in Fig. \ref{fig:slamOverview}, is that localizations requires a map and mapping depends on pose estimates however neither are certain.

%The following sections will mainly focus on the SLAM back-end and dissect the problem by reviewing different probabilistic approaches. 


%In order to partially solve this dilemma SLAM incorporates loop closure to correct the global map as well as positional estimates provided by odometry. A loop closure event occurs when revisiting previously mapped landmarks and associating relations between features to adapt the global topology. %Without loop closure a mobile robot would perceive its environment as a infinite corridor.


\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{img/slamOverview}
	\caption{
		The figure above depicts the different tasks contributing to the SLAM problem. Panel a) shows a robot moving trough a known environment while the pose and path remain uncertain. In panel b) the robots pose is certain while the landmark locations need to be determined. Panel c) composes both previous problems where neither the robot state nor the landmark locations are certain.
	}
	\label{fig:slamOverview}
\end{figure}

\section{Map Representation}
The way a robot perceives its environment and maintains a accurate map of its environment is directly dependant on many criteria such as complexity of tasks, size of its environment as well as measurement quality mainly influenced by sensor noise. 
Tailoring to benefit some of the aforementioned criteria most robotic mapping systems utilize either of two paradigms, metric or topological, each proposing their respective strengths and weaknesses.

Metric or grid-based maps build a map of the robots environment as occupancy grids, with each grid cell indicating the presence of an obstacle. The main benefit using metric maps is the facilitated construction of large-scale mappings as well as non-ambiguous determination of places. However such maps have significant drawbacks concerning space as well as time complexity and require accurate pose estimation of the robot.

Topological or feature-based maps reconstruct their environment as graphs with each node representing a feature or landmark perceived by the robots sensors. In contrast to metric maps they allow for comprehensive path planning and are significantly more compact as its resolution is directly proportional to the environments complexity. \footcite{thrunMaps1998}


\section{Probabilistic Definition}
To describe probabilistic SLAM lets assume a robot is moving through an environment observing multiple landmarks at different times. 
The goal is that, for any time instant $ k $, the robots state vector 
\[ x_{k} = 
\begin{pmatrix}
	x \\
	y \\
	\theta \\
\end{pmatrix}
\] describing its position and orientation as well as a time invariant set of all absolute landmark positions 
$ m = \{ m_{1}, m_{2}, ..., m_{i} \}$ 
is computed, given uncertain relative landmark observations $ Z_{0:k} = \{z_{0}, z_{1}, ..., z_{k}\}$, controls $ U_{0:k} = \{u_{0}, u_{1}, ..., u_{k}\}$ and known initial location $ x_{0} $. Fig. \ref{fig:slamGraphical} depicts this process and illustrates the relationships between the variables important in any SLAM system. 

\begin{equation}\label{fullSLAM}
	P(x_{k:0}, m | Z_{0:k}, U_{0:k}, x_{0})
\end{equation}

In \ref{fullSLAM}, which is often referred to as "Full-SLAM" or "Offline-SLAM", the joint posterior density of the robots trajectory and landmark locations is estimated. In other words, this formulation of the problem aims to recover the whole robot trajectory.

\begin{equation}\label{onlineSLAM}
	P(x_{k}, m | Z_{0:k}, U_{0:k}, x_{0})
\end{equation}

The pendant to this is "Online-SLAM" which aims estimate only the robots latest location at time instant $ k $, as seen in \ref{onlineSLAM}. In contrast to "Full-SLAM", performing batch computation on the whole data, algorithms pursuing this online approach typically compute the probability distribution incrementally. 

\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{img/SlamGraphical}
	\caption{
		A Bayes network graph depicting the causal relationships between sensor measurements (shaded circles) and uncertain variables (blank circles). It is made apparent how the relative measurements by odometry or landmark observation influence the robot state as well as landmark locations. 
	}
	\label{fig:slamGraphical}
\end{figure}

To compute either problem a mathematical model representing the robots state transition and a model describing observations need to be defined:

\begin{equation}\label{motionModel}
	P(x_{k} | x_{k-1}, u_{k})
\end{equation}

\begin{equation}\label{observationModel}
	P(z_{k} | x_{k}, m)
\end{equation}

The motion model \ref{motionModel} describes the robots location at time $ k $ given a known previous location $ x_{k-1} $ and odometry $ u_{k} $ assuming the state-transition is a Markov process \footcite{haeneltMarvoModel2006}.
\ref{observationModel} calculates the probability of observing a landmark $ z_{k} $ when the robots location $ x_{k} $ and environment $ m $ are known. For aforementioned reasons in SLAM the map is not known to the system, therefore many formulations omit the map in the observation model.

%The joint posterior is then implemented in a recursive prediction and correction form which is often described as time-update and measurement-update:

For filtering approaches which are discussed later in this chapter, the joint posterior is then implemented in a recursive prediction and correction form which is often described as time-update and measurement-update:

\begin{equation}\label{timeUpdate}
	P(x_{k}, m | Z_{0:k-1}, U_{0:k}, x_{0}) = \int P(x_{k} | x_{k-1}, u_{k}) * P(x_{k-1}, m | Z_{0:k-1}, U_{0:k-1}, x_{0}) dx_{k-1}
\end{equation}


\begin{equation}\label{measurementUpdate}
	P(x_{k}, m | Z_{0:k}, U_{0:k}, x_{0}) = \frac{P(z_{k} | x_{k}, m) * P(x_{k}, m | Z_{0:k-1}, U_{0:k}, x_{0}))}{P(z_{k} | Z_{0:k-1}, U_{0:k})}
\end{equation}

The time update in \ref{timeUpdate} calculates an estimate of the robots state using a previously known state at $ k - 1 $ and state-transition given the newest control $ u_{k} $.
In the next step, the measurement update \ref{measurementUpdate}, a observation $z_{k} $ is taken and the previously calculated estimate is corrected using Bayes-Theorem. 

To make the the recursive structure of such filtering approaches more apparent the equations \ref{timeUpdate} and \ref{measurementUpdate} can also be described in Bayesian form as follows:

\begin{equation}\label{timeUpdateBayes}
	\overline{\rm bel}(x_{k}) = \int P(x_{k} | x_{k-1}, u_{k}) * bel(x_{k-1}) dx_{k-1}
\end{equation}


\begin{equation}\label{measurementUpdateBayes}
	bel(x_{k}) = \eta * P(z_{k} | x_{k}) * \overline{\rm bel}(x_{k-1})
\end{equation}

\section{Solution Paradigms} 
Over time many different approaches to the SLAM problem have been developed which can be categorized into either of three paradigms. The two mature ones are based on recursive filtering techniques while the modern graph-based solutions perform non-linear sparse optimization.

\subsection{Extended Kalman Filter}
The Extended Kalman Filter (EKF) is one of the first approaches to the Online-SLAM problem and similar to the standard Kalman Filter one implementation of a Bayes Filter. Contrary to the Kalman Filter it is applicable to non-linear systems by performing linear approximation.

%The Extended Kalman Filter (EKF) is one implementation of a Bayes Filter for non-linear problems and was one of the first approaches to the Online-SLAM problem. The author recommends preliminary knowledge about Bayes Filters\footcite{tipaldiBayes2020}, the vanilla Kalman Filter and the EKF\footcite{tipaldiEKF2020} before reading this section about its application in SLAM. 

The Kalman Filter assumes a environment in which landmarks can be fully represented as points within the coordinate frame. The earlier introduced state-vector $ x $ is extended to a state-space vector with dimensions $ 3 + 2N$
\[ \mu = 
\begin{pmatrix}
	x \\
	m \\
\end{pmatrix}
=
\begin{pmatrix}
	x & y & \theta & m_{1, x} & m_{1, y} & \dots & m_{n, x} & m_{n, y} 
\end{pmatrix} ^{T}
\] 
to incorporate the landmark locations $ m_{i, x} $ and $ m_{i, y} $. 

Furthermore a covariance matrix 
$ \Sigma = 
\begin{pmatrix}
	\Sigma_{xx} & \Sigma_{xm} \\
	\Sigma_{mx} & \Sigma_{mm} \\
\end{pmatrix} $ 
is introduced which specifies the state $ x $ and landmark $ m $ uncertainties as well as correlations between landmarks and robot pose. 

The motion and observation models are defined as non-linear Gaussian distributions $ \mathcal{N}(g(u_{k}, \mu_{k - 1}), R_{k}) $ and $ \mathcal{N}(h(x_{k}, m), Q_{k}) $ with $ R_{k} $ and $ Q_{k} $ representing zero-mean additive Gaussian noise.
To still calculate the state transition as well as measurement update these functions are linearized by calculating their Jacobian matrices $ G_{k} $ and $ H_{k} $.

This allows for the functions to be incorporated in the vanilla Kalman Filter Algorithm which consists of the following five step procedure:

\begin{lstlisting}[mathescape=true, caption={Pseudo-Code describing the filter cycle of a Extenden Kalman Filter},captionpos=b,label={lst:kalman_filter}]
	def extended_kalman_filter($\mu_{k-1}$, $\Sigma_{k-1}$, $u_{k}$, $z_{k}$):
	$\overline{\mu}_{k} = g(u_{k}, \mu_{k-1})$				# state-prediction given newest control measurements
	$\overline{\Sigma}_{k} = G_{k}\Sigma_{k-1}G_{k-1}^{T} + R_{k}$				# measurement prediction 	 		
	
	$K_{k} = \overline{\Sigma}_{k}H_{k}^{T}(H_{k}\overline{\Sigma}_{k}H_{k}^{T} + Q_{k})^{-1}$				# measurement of actual landmark position
	$\mu_{k} = \overline{\mu}_{k} + K_{k}(z_{k}-h(\overline{\mu}_{k}))$				# data association 
	$\Sigma_{k} = (I - K_{k}H_{k})\overline{\Sigma}_{k}$				# update of state-space vector as well as covariance matrix 
	
	return $\mu_{k}, \Sigma_{t}$
\end{lstlisting}

%\begin{itemize}
%	\item State Prediction: The next state $ \mu_{k} $ is predicted given the controls $ u_{k} $ and previous state $ \mu_{k-1} $.
%	\item Measurement Prediction: A prediction of the next measurement is made propagating the uncertainty introduced by the state-transition.
%	\item Measurement: A new measurement is taken.
%	\item Data Association: The error between the prediction and actual measurement is calculated.
%	\item Update: The state-space vector as well as covariance matrix is updated given the previously calculated error. This update then reduces the uncertainty of both robot state and landmark locations.
%\end{itemize} 


%wenn noch platz bleibt dar√ºber schreiben dass taylor expansion nicht so gut ist

\subsection{Particle Filter}
One major problem with the EKF paradigm is its inability to perform estimations under non-Gaussian distributions. 
To this end particle filters estimate a specific probability distribution by proposing multiple possible hypothesis which get continuously more accurate as the algorithm proceeds. 
Particle filtering for SLAM with algorithms like FastSLAM, shares many similarities with Monte-Carlo localization which is roughly described in the following steps:
\begin{itemize}
	\item For each state $ x_{k} $ a set 
	$
	\mathcal{X} = 
	\{ 
	\langle
	\begin{matrix}
		 x_{k}^{[j]},  \omega^{[j]}
	\end{matrix}
	\rangle
	, ...
	\}
	_{j=1, ..., J}
	$
	particles or samples 
	with random state hypothesis is generated from a proposal distribution. 
	\item The probability for each particle to be representing the true state is calculated by comparing its estimation with a measurement thus yielding in each particles importance weight $ \omega^{[j]} $.
	\item The last step is to remove samples with a lower likelihood of being correct and add more samples in regions with higher importance weights, this process is called resampling and shares many similarities with survival of the fittest algorithms.
\end{itemize}

%nur kleiner satz der mir eingefallen ist
Because the number of particles greatly affects the performance of a particle filter, such approaches perform best in low dimension spaces. To this end the FastSLAM algorithm utilizes Rao-Blackwellization, see Equation \ref{RaoBlackwellization}, to separate the joint probability of robot state and map into individual distributions which can be computed with much more ease.
This means that given the robots poses all landmarks are independent, which is described in Fig. \ref{fig:fastSlamGraphical}.

\begin{equation}\label{RaoBlackwellization}
	P(x_{0:k}, m | Z_{0:k}, U_{0:k}) = P(x_{0:k} | Z_{0:k}, U_{0:k}) P(m | x_{0:k}, Z_{0:k})
\end{equation}

Following this approach the robots path is estimated by performing a Monte-Carlo localization drawing samples from the robots motion model $x_{t}^{[j]} \sim P(x_{k} | x_{k-1}^{[j]}, u_{k}) $. Besides the state information each particle maintains a set of landmarks, each represented as a low dimensional EKF. Therefore the importance weight for each particle corresponds to the accuracy between expected measurement and observation as $ \omega^{[j]} \sim P(z_{k} | x_{k}^{[j]}, \overline{z}_{k}) $.
% therefore the importance weight for each particle corresponds to the joint accuracy of all landmark estimations. 
The final algorithm functions exactly as described with the exception that it incorporates the aforementioned EKF to estimate the landmark locations and compute the particle weight before updating its belief on landmark locations and initiating the resampling process. 


\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{img/FastSlamGraphical}
	\caption{
		This graphical description of the SLAM problem shows the assumption that the landmark estimates are all mediated through the robot path thus allowing for easier implementation by using smaller Gaussians for each landmark $m_{i}$ rather than a larger state-space vector as in EKF SLAM.
	}
	\label{fig:fastSlamGraphical}
\end{figure}

\subsection{Graph-based}
The newest paradigm to solve the offline SLAM problem are graph-based methods that essentially use a non-linear least-squares approach to compute the graph of poses best fitting the measurements. 
To this end the SLAM posterior is described as a graph of robot poses $ x_{i} $ at time $ k_{i} $ as nodes connected by soft constraints that correspond to spatial measurements between two states.
These edges are generated in the following two cases:

\begin{equation}\label{odometryEdge}
	(X{i}^{-1}X_{i+1})
\end{equation}

\begin{equation}\label{observationEdge}
	(X_{i}^{-1}X_{j})
\end{equation}

Here \ref{odometryEdge} corresponds to a edge based on odometry transformations by moving the robot from state $X_{i}$ to $X_{i+1}$, both of which are described as transformation matrices relative to the coordinate origin.
These transformations are illustrated in more detail in Fig. \ref{fig:poseGraphTransformation}.
The more interesting case is when an observation of a previously captured area occurs and a virtual measurement \ref{observationEdge} is computed, describing how the states $X_{j}$ and $X_{i}$ relate to each other according to the displacement between their observations, see Fig. \ref{fig:poseGraphOptimization}.

\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{img/PoseGraphTransformation}
	\caption{
		The graph of robot poses connected to each other by odometry measurements. To calculate the relative movement between two nodes each node is represented as a matrix $X_{i}$ describing how it is transformed relative to the global coordinate frame. 
	}
	\label{fig:poseGraphTransformation}
\end{figure}



This approach marginalizes out any data about the map other than the correspondence of poses to primarily focus on solving the robot path.  
Given the pose-graph the goal is to compute the node configuration that minimizes the error between predicted edges, see \ref{observationEdge}, and actual measurements 
$
\langle
\begin{matrix}
	z_{ij},  \Omega_{ij}
\end{matrix}
\rangle
$, with the information matrix $\Omega_{ij}$ describing the measurement noise.

Equation \ref{leastSquares} describes this optimization process, with the error function $e_{ij}$ defined as any non-linear function essentially calculating the difference between estimate and true measurement for any pair of nodes. 
Once the optimal pose-graph is constructed the map data is rendered given known poses thus reducing the problem to simple mapping.

\begin{equation}\label{leastSquares}
	x* = argmin_{x} \sum_{ij}e_{ij}^{T} \Omega_{ij} e_{ij}
\end{equation}


\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{img/PoseGraphOptimization}
	\caption{
		This pose-graph with two nodes describes how any pair of nodes are related to each other given a measurement. The measurement $ z_{ij} $ corresponds to how the node $x_{j}$ should be placed relative to node $x_{i}$ given that each of them share a similar observation. As seen in the figure this is not the case, thus demanding for the error $e_{ij}$ to be minimized. In other words, by moving the node $x_{j}$ closer to the optimum minimizes the error and corrects the robot state. 
	}
	\label{fig:poseGraphOptimization}
\end{figure}


\section{Graph-SLAM topology}
%!!!!!! Front-End Backend Trennung ist nur bei graph based der Fall
Most modern graph-based SLAM systems are partitioned into the following modules:
\begin{itemize}
	\item A front-end in charge of abstracting sensory input by performing feature extraction as well as data association, see Section \ref{dataAssociation}. Essentially it will construct the pose-graph and feed it into the back-end.
	%Furthermore it is responsible for associating corresponding features over multiple measurements, which is typically referred to as short-term data association. Long-term data association or loop closure therefore aims to link current measurements to previously observed features thus  correcting accumulated odometry errors as well as adapting the global topology. 
	\item A back-end performing pose-graph optimization based on the abstracted data fed in by the front-end. The back-end also provides positional information about nodes to the front-end thus supporting loop closure detection. 
\end{itemize}

While the back-end can be generalized for many different nuances of the SLAM problem, the front-end remains application specific as the data abstraction depends on the sensors utilized by the robot.

%and is implemented by least-square solving algorithms such as gradient decent 

%auch in section graph based schieben
\section{Data Association}\label{dataAssociation}
%While many aspects of the front-end remain application specific each SLAM system typically contains modules for data association in the two ways.
Short-term data association or feature tracking is responsible for associating corresponding features over multiple measurements while long-term data association or loop-closure aims to detect and link similarities between current measurements and previously observed features. These loops help correcting accumulated odometry errors as well as optimizing the global topology. 
A loop closure typically event occurs when revisiting previously mapped landmarks and is computed by scan-matching algorithms.

\section{Visual SLAM}


%\section{Application Scenarios}
%
%\subsection{Map Acquisition}
%
%\subsection{Black-Box Localization}
%
%\subsection{Continuos Map Exploration}
%
%
%\section{2D SLAM}
%
%\subsection{Evidence Grid-based SLAM}
%
%\subsection{Graph-based SLAM}
%
%\subsection{Feature-based SLAM}
%
%
%\section{3D SLAM}
%
%\subsection{Visual SLAM}
%
%\subsection{Graph-based SLAM}
%
%
%\section{Obtaining Odometry}
%
%\subsection{Wheel Encoder Odometry}
%
%\subsection{Visual Odometry}
%
%\subsection{Visual Inertial Odometry}


\section{Loop Closure for Visual SLAM}

\subsection{Appearance-based}

\subsection{Deep Learning}


\section{Map optimization}

\subsection{Bundle Adjustment}



\filbreak